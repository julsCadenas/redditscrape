{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL USING ROGUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\n",
    "            \"summarization\",\n",
    "            model = \"../models/redditsummary\",\n",
    "            tokenizer = \"../models/redditsummary\",\n",
    "        )\n",
    "        \n",
    "def summarize(text, prompt):\n",
    "    inputs = f\"{prompt}: {text}\"\n",
    "    input_tokens = summarizer.tokenizer.encode(inputs, truncation=False)\n",
    "    input_len = len(input_tokens)\n",
    "    max_length = min(input_len * 2, 1024)\n",
    "    min_length = max(32, input_len // 4)\n",
    "    summary = summarizer(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "    )\n",
    "    return summary[0]['summary_text']\n",
    "    \n",
    "def process_data(response, prompt):\n",
    "    post_content = response[0]['data']['children'][0]['data'].get('selftext', '')\n",
    "    comments = []\n",
    "    for comment in response[1]['data']['children']:\n",
    "        if 'body' in comment['data']:\n",
    "            comments.append(comment['data']['body'])\n",
    "    comments_all = ' '.join(comments)\n",
    "\n",
    "    post_summary = summarize(post_content, prompt)\n",
    "    comments_summary = summarize(comments_all, prompt)\n",
    "\n",
    "    return {\n",
    "        \"post_summary\": post_summary,\n",
    "        \"comments_summary\": comments_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the reddit post and summarize it then save the summary in another json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the reddit post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/response.json') as file:\n",
    "    reddit_post = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "process the summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 274, but your input_length is only 137. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=68)\n",
      "Your max_length is set to 380, but your input_length is only 190. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=95)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary saved to summary.json\n"
     ]
    }
   ],
   "source": [
    "summary = process_data(reddit_post, \"Summarize and highlight popular brands\")\n",
    "\n",
    "with open('../data/summary.json', 'w') as file:\n",
    "    json.dump(summary, file, indent=4)\n",
    "\n",
    "print(\"Summary saved to summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verify by printing the summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Summary:\n",
      " {\"title\": \"Budget for a New Chair with Adjustable Headrest and Armrest\", \"selftext\": \"The user is looking for a new chair with an adjustable headrest, armrest, chair height, and a lumbar pillow with mesh material. They mention a budget of P3,000-P4,000 and highlight popular brands.\", \"comments\": [\"Users provide feedback on the chair's specifications and suggest alternative options.\", \"Some users suggest reaching out to influencers for recommendations on speakerphones or headphones for better long-distance communication.\", \"The sentiment is positive, with users appreciating the user's effort to find a chair and offering helpful suggestions for its design.\"], \"sentiment\": \" the sentiment is encouraging and supportive, with Users sharing their own search experiences and offering suggestions for additional features on chairs and speakers.\"}\n",
      "\n",
      "Comments Summary:\n",
      " {\"title\": \"Budget-friendly office chair suggestions\", \"selftext\": {\"overview\": \"The post provides a budget-friendly option for purchasing a used desktop chair, highlighting popular brands like Steelcase and Sihoo as examples. The user shares their own experience with buying good chairs under budget, highlighting the benefits of buying used or refurbished models.\", \"suggestions\": [\"Used Steelcase chairs priced at around 4k PHP on Facebook Marketplace with a guarantee.\", \"Carousell offers office chairs for around 1.5k PHP, which is within the budget.\", \"The OP shares a positive experience with purchasing a SihOO m57 and an Ergo Prime Mesh chair for around 8-9k PHP each, describing them as the best desktop chairs they've ever bought.\", \"There are 3-5k chairs out there, and the OP advises trying different platforms to find good deals.\"], \"comments\": [\"Users share their own experiences with buying chairs and advise the OP on where to buy and how to proceed.\", \"Some users mention limited budget options and recommend specific brands or platforms based on their previous experiences.\", \" The sentiment is mixed, with users appreciating the OP's experience but also sharing their own concerns about the quality of used chairs and the wide variety of models available.\"]}}\n"
     ]
    }
   ],
   "source": [
    "print(\"Post Summary:\\n\", summary[\"post_summary\"])\n",
    "print(\"\\nComments Summary:\\n\", summary[\"comments_summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix the json files (the output summary files are not formatted properly (json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial summary\n",
    "with open('../data/summary.json', 'r') as file:\n",
    "    generated_summaries = file.read()\n",
    "\n",
    "# initial reference\n",
    "with open('../data/reference.json', 'r') as file:\n",
    "    reference_summaries = file.read()\n",
    "with open('../data/reference2.json', 'r') as file:\n",
    "    reference_summaries2 = file.read()\n",
    "with open('../data/reference3.json', 'r') as file:\n",
    "    reference_summaries3 = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to fix the formatting of the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_json(jsonfile, path):\n",
    "    # If jsonfile is already a dictionary, we don't need to load it as JSON again\n",
    "    if isinstance(jsonfile, str):\n",
    "        improper_json = jsonfile\n",
    "        fixed_json = json.loads(improper_json)\n",
    "    else:\n",
    "        fixed_json = jsonfile  # already a dictionary\n",
    "\n",
    "    # Fixing the nested JSON fields\n",
    "    fixed_post_summary = json.loads(fixed_json['post_summary']) if isinstance(fixed_json['post_summary'], str) else fixed_json['post_summary']\n",
    "    fixed_comments_summary = json.loads(fixed_json['comments_summary']) if isinstance(fixed_json['comments_summary'], str) else fixed_json['comments_summary']\n",
    "\n",
    "    fixed_json['post_summary'] = fixed_post_summary\n",
    "    fixed_json['comments_summary'] = fixed_comments_summary\n",
    "\n",
    "    # Printing the fixed JSON to check\n",
    "    print(json.dumps(fixed_json, indent=4))\n",
    "\n",
    "    # Saving the fixed JSON to a file\n",
    "    with open(path, 'w') as file:\n",
    "        json.dump(fixed_json, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fix the formatting of the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"post_summary\": {\n",
      "        \"title\": \"Budget for a New Chair with Adjustable Headrest and Armrest\",\n",
      "        \"selftext\": \"The user is looking for a new chair with an adjustable headrest, armrest, chair height, and a lumbar pillow with mesh material. They mention a budget of P3,000-P4,000 and highlight popular brands.\",\n",
      "        \"comments\": [\n",
      "            \"Users provide feedback on the chair's specifications and suggest alternative options.\",\n",
      "            \"Some users suggest reaching out to influencers for recommendations on speakerphones or headphones for better long-distance communication.\",\n",
      "            \"The sentiment is positive, with users appreciating the user's effort to find a chair and offering helpful suggestions for its design.\"\n",
      "        ],\n",
      "        \"sentiment\": \" the sentiment is encouraging and supportive, with Users sharing their own search experiences and offering suggestions for additional features on chairs and speakers.\"\n",
      "    },\n",
      "    \"comments_summary\": {\n",
      "        \"title\": \"Budget-friendly office chair suggestions\",\n",
      "        \"selftext\": {\n",
      "            \"overview\": \"The post provides a budget-friendly option for purchasing a used desktop chair, highlighting popular brands like Steelcase and Sihoo as examples. The user shares their own experience with buying good chairs under budget, highlighting the benefits of buying used or refurbished models.\",\n",
      "            \"suggestions\": [\n",
      "                \"Used Steelcase chairs priced at around 4k PHP on Facebook Marketplace with a guarantee.\",\n",
      "                \"Carousell offers office chairs for around 1.5k PHP, which is within the budget.\",\n",
      "                \"The OP shares a positive experience with purchasing a SihOO m57 and an Ergo Prime Mesh chair for around 8-9k PHP each, describing them as the best desktop chairs they've ever bought.\",\n",
      "                \"There are 3-5k chairs out there, and the OP advises trying different platforms to find good deals.\"\n",
      "            ],\n",
      "            \"comments\": [\n",
      "                \"Users share their own experiences with buying chairs and advise the OP on where to buy and how to proceed.\",\n",
      "                \"Some users mention limited budget options and recommend specific brands or platforms based on their previous experiences.\",\n",
      "                \" The sentiment is mixed, with users appreciating the OP's experience but also sharing their own concerns about the quality of used chairs and the wide variety of models available.\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"post_summary\": {\n",
      "        \"title\": \"Best budget-friendly Ergonomic Chair with Adjustable Headrest, Armrest and Lumbar Pillow\",\n",
      "        \"selftext\": \"The user is looking for a new ergonomic chair around P3,000-P4,000 with an adjustable headrest, armrest and lumbar pillow made with mesh material.\",\n",
      "        \"comments\": \"Users suggested alternative options and suggested that the user should try buying secondhand from stores like Carousel and FB Marketplace.\",\n",
      "        \"sentiment\": \"The sentiment is positive with users are supporting and offering suggestions to the OP. The sentiment is also encouraging as the users shared their own experiences in the comments.\"\n",
      "    },\n",
      "    \"comments_summary\": {\n",
      "        \"title\": \"Budget-friendly Ergonomic Chair Suggestions\",\n",
      "        \"selftext\": {\n",
      "            \"overview\": \"The comments suggested to the OP to buy used chairs, highlighting popular brands like Steelcase and Sihoo as examples.\",\n",
      "            \"suggestions\": [\n",
      "                \"Used Steelcase chairs around 4k PHP on Facebook Marketplace\",\n",
      "                \"Check Carousel out for office chairs around 1.5K\",\n",
      "                \"Sihoo m57 and Ergo Prime Mesh are the best but are priced around 8-9k.\",\n",
      "                \"There are cheap chairs around 3-5k but they don't last.\"\n",
      "            ],\n",
      "            \"comments\": [\n",
      "                \"Users share their own experiences with buying chairs and advised the OP on where to buy.\",\n",
      "                \"The comments also recommended specific brands and platforms based on their previous experiences.\",\n",
      "                \"The sentiment is mixed as some users shared positive experiences while others are unsure and didn't like the quality of used and cheaper chairs.\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"post_summary\": {\n",
      "        \"title\": \"Best budget-friendly Ergonomic Chair with Adjustable Headrest, Armrest and Lumbar Pillow for WFH\",\n",
      "        \"selftext\": \"The user is looking for a new ergonomic chair around P3,000-P4,000 with an adjustable headrest, armrest and lumbar pillow made with mesh material for WFH.\",\n",
      "        \"comments\": \"Commenters suggested alternative options like Sihoo M57 and Steelcase and suggested that the user should try buying secondhand from stores like Carousel and FB Marketplace.\",\n",
      "        \"sentiment\": \"The sentiment is positive with users supporting and suggesting alternatives to the OP as the users shared their own experiences in buying ergonomic chairs.\"\n",
      "    },\n",
      "    \"comments_summary\": {\n",
      "        \"title\": \"Budget-friendly Ergonomic Chair Suggestions\",\n",
      "        \"selftext\": {\n",
      "            \"overview\": \"The comments suggested to the OP to buy secondhand chairs as cheaper chairs tend to be lower in quality, the comments also highlighted popular brands like Steelcase and Sihoo as examples.\",\n",
      "            \"suggestions\": [\n",
      "                \"Secondhand chairs around 4k PHP on Facebook Marketplace\",\n",
      "                \"Check Carousel out for office chairs around 1.5K\",\n",
      "                \"Out of the price range but Sihoo m57 and Ergo Prime Mesh are the best but are priced around 8-9k.\",\n",
      "                \"Cheap chairs priced around 3-5k but they don't last.\"\n",
      "            ],\n",
      "            \"comments\": [\n",
      "                \"Users share their own experiences with buying ergonomic chairs and advised the OP on places where to buy.\",\n",
      "                \"The comments also recommended specific brands like Sihoo and where to buy such as the FB Marketplace.\",\n",
      "                \"The sentiment is mixed as some commenters shared positive experiences while others are unsure with the quality of used and cheaper chairs.\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"post_summary\": {\n",
      "        \"title\": \"Best budget-friendly Mesh Ergonomic Chairs with Adjustable Headrest, Armrest and Lumbar Pillow for WFH\",\n",
      "        \"selftext\": \"The user is asking for suggestions for a new mesh ergonomic chair around P3,000-P4,000 with an adjustable headrest, armrest and lumbar pillow for WFH.\",\n",
      "        \"comments\": \"Commenters suggested alternative and secondhand options like Sihoo M57 and Steelcase from Carousel and FB Marketplace.\",\n",
      "        \"sentiment\": \"The sentiment is positive with users sharing their own experiences in buying ergonomic chairs and suggesting alternative options to the OP.\"\n",
      "    },\n",
      "    \"comments_summary\": {\n",
      "        \"title\": \"Budget-friendly Ergonomic Chair Suggestions for WFH\",\n",
      "        \"selftext\": {\n",
      "            \"overview\": \"The commenters suggested to the OP to buy secondhand Sihoo or Steelcase chairs as cheaper chairs within the stated budget tend to be lower in quality.\",\n",
      "            \"suggestions\": [\n",
      "                \"Buy Secondhand Steelcase chairs around 4k PHP on Facebook Marketplace\",\n",
      "                \"There are office chairs around 1.5K in Carousel\",\n",
      "                \"Out of the user's budget but Sihoo M57 and Ergo Prime Mesh chairs are the best.\",\n",
      "                \"Chairs priced around 3-5k typically don't last long\"\n",
      "            ],\n",
      "            \"comments\": [\n",
      "                \"Users share their own experiences with buying ergonomic chairs and advised the OP to buy secondhand.\",\n",
      "                \"The comments also recommended specific brands like Sihoo, Ergo Prime and Steelcase and where to buy such as the FB Marketplace.\",\n",
      "                \"The sentiment is mixed as commenters shared positive experiences while others complained about the quality of cheaper chairs.\"\n",
      "            ]\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fix_json(generated_summaries, '../data/summary_fixed.json')\n",
    "fix_json(reference_summaries, '../data/reference_fixed.json')\n",
    "fix_json(reference_summaries2, '../data/reference_fixed2.json')\n",
    "fix_json(reference_summaries3, '../data/reference_fixed3.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open the fixed summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed summary\n",
    "with open('../data/summary_fixed.json', 'r') as file:\n",
    "    generated_summaries_fixed = file.read()\n",
    "\n",
    "# fixed reference\n",
    "with open('../data/reference_fixed.json', 'r') as file:\n",
    "    reference_summaries_fixed = file.read()\n",
    "with open('../data/reference_fixed2.json', 'r') as file:\n",
    "    reference_summaries_fixed2 = file.read()\n",
    "with open('../data/reference_fixed3.json', 'r') as file:\n",
    "    reference_summaries_fixed3 = file.read()\n",
    "    \n",
    "# print(generated_summaries_fixed)\n",
    "# print(reference_summaries_fixed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_refs = [reference_summaries_fixed, reference_summaries_fixed2, reference_summaries_fixed3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate the results using ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: [{'rouge-1': {'r': 0.7007299270072993, 'p': 0.5245901639344263, 'f': 0.5999999951033204}, 'rouge-2': {'r': 0.42790697674418604, 'p': 0.2948717948717949, 'f': 0.3491461052263178}, 'rouge-l': {'r': 0.6788321167883211, 'p': 0.5081967213114754, 'f': 0.5812499951033204}}]\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.6056338028169014, 'p': 0.46994535519125685, 'f': 0.5292307643103433}, 'rouge-2': {'r': 0.32894736842105265, 'p': 0.2403846153846154, 'f': 0.27777777289876554}, 'rouge-l': {'r': 0.5845070422535211, 'p': 0.453551912568306, 'f': 0.5107692258488049}}]\n",
      "ROUGE Scores: [{'rouge-1': {'r': 0.6370370370370371, 'p': 0.46994535519125685, 'f': 0.5408804982585738}, 'rouge-2': {'r': 0.3188405797101449, 'p': 0.21153846153846154, 'f': 0.25433525532025797}, 'rouge-l': {'r': 0.6074074074074074, 'p': 0.44808743169398907, 'f': 0.5157232655541711}}]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for ref in all_refs:\n",
    "    result = rouge.get_scores(generated_summaries_fixed, ref)\n",
    "    results.append(result)\n",
    "\n",
    "print(f\"ROUGE Scores: {results[0]}\")\n",
    "print(f\"ROUGE Scores: {results[1]}\")\n",
    "print(f\"ROUGE Scores: {results[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ROUGE SCORES**\n",
    "\n",
    "| Metric    | Recall (%) | Precision (%) | F1-Score (%) |\n",
    "|-----------|------------|---------------|--------------|\n",
    "| **ROUGE-1** | 57.66      | 43.41         | 49.53        |\n",
    "| **ROUGE-2** | 29.30      | 20.72         | 24.27        |\n",
    "| **ROUGE-L** | 56.20      | 42.30         | 48.28        |\n",
    "\n",
    "\n",
    "### **CONCLUSIONS**\n",
    "**ROGUE-1 SCORE:** The ROUGE-1 score indicates that the model is fairly good at capturing individual words (unigrams) from the reference summaries. With a recall of 57.66%, the model captures more than half of the relevant words from the reference summaries, which suggests that it's effectively capturing key content. The precision of 43.41% indicates that a significant portion of the generated summary’s words also appear in the reference, but there may be some additional, irrelevant words included. The F1-score of 49.53% shows that, overall, there’s a fairly good balance between recall and precision, although there's still room to increase both aspects for better results.\n",
    "\n",
    "**ROGUE-2 SCORE:** The ROUGE-2 score, which focuses on bigram overlap, is considerably lower than ROUGE-1. The recall of 29.30% indicates that the model captures roughly 30% of the bigrams from the reference summaries, which is a moderate result but suggests that the model may not be fully preserving the structural relationships between words. The precision of 20.72% suggests that the generated summaries might include bigrams that are not present in the reference summaries. The F1-score of 24.27% is relatively low, which may indicate that the model needs improvement in capturing bigram patterns in the summaries. This is common in summarization tasks, as producing high-quality bigram overlap is challenging.\n",
    "\n",
    "**ROGUE-l SCORE:** The ROUGE-L score, which focuses on the longest common subsequence (LCS), shows that the model is able to capture the overall structure of the reference summaries quite well. The recall of 56.20% suggests that a large portion of the key sequences (order-preserving) from the reference summaries appear in the generated summaries, indicating good coherence. The precision of 42.30% shows that the model does well in maintaining relevant sequences in the generated summary but could further reduce redundant or non-informative sequences. The F1-score of 48.28% indicates a solid performance in preserving the flow and structure of the original text.\n",
    "\n",
    "### **SUMMARY**  \n",
    "- The **ROUGE-1** score is strong, indicating that the model is capturing individual words well, which is important for summarizing the key points of Reddit posts. This suggests the model is effectively identifying the core content from the original Reddit discussions.\n",
    "\n",
    "- The **ROUGE-2** score is relatively low, suggesting that the model struggles with preserving the structure and sequence of words, which is crucial for generating coherent summaries of Reddit posts where sentence flow and the connection between ideas are important.\n",
    "\n",
    "- The **ROUGE-L** score shows that the model is effectively capturing meaningful sequences and maintaining coherence in the summaries. This is a positive outcome for summarizing Reddit posts, where keeping the overall message and flow intact is important.\n",
    "\n",
    "While the model performs well in certain areas (particularly with ROUGE-1 and ROUGE-L), there is room for improvement, especially with the ROUGE-2 score. Improving bigram overlap could enhance the fluency and structure of the summaries, leading to more readable and coherent summaries of Reddit posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the model using BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the dataset for BERTScore evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_len(generated_summaries, reference_summaries):\n",
    "    if not isinstance(generated_summaries, list):\n",
    "        generated_summaries = [generated_summaries]  \n",
    "    if not isinstance(reference_summaries, list):\n",
    "        reference_summaries = [reference_summaries] \n",
    "    \n",
    "    if isinstance(reference_summaries[0], str):  \n",
    "        reference_summaries = [[ref] for ref in reference_summaries]\n",
    "    \n",
    "    if len(generated_summaries) > len(reference_summaries):\n",
    "        reference_summaries += [[\"\"]] * (len(generated_summaries) - len(reference_summaries))\n",
    "    elif len(reference_summaries) > len(generated_summaries):\n",
    "        generated_summaries += [\"\"] * (len(reference_summaries) - len(generated_summaries))\n",
    "    \n",
    "    return generated_summaries, reference_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_summaries, reference_summaries_fixed = check_len(generated_summaries_fixed, reference_summaries_fixed)\n",
    "generated_summaries, reference_summaries_fixed2 = check_len(generated_summaries_fixed, reference_summaries_fixed2)\n",
    "generated_summaries, reference_summaries_fixed3 = check_len(generated_summaries_fixed, reference_summaries_fixed3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_refs_bert = []\n",
    "for ref1, ref2, ref3 in zip(reference_summaries_fixed, reference_summaries_fixed2, reference_summaries_fixed3):\n",
    "    all_refs_bert.append(ref1 + ref2 + ref3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertScore: P=tensor([0.9268]), R=tensor([0.9330]), F1=tensor([0.9299])\n"
     ]
    }
   ],
   "source": [
    "P, R, F1 = score(generated_summaries, all_refs_bert, lang='en')\n",
    "\n",
    "print(f\"BertScore: P={P}, R={R}, F1={F1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BERTScore**\n",
    "\n",
    "| Metric    | Value (%) |\n",
    "|-----------|-----------|\n",
    "| **Recall**    | 90.85     |\n",
    "| **Precision** | 92.44     |\n",
    "| **F1-Score**  | 91.64     |\n",
    "\n",
    "#### **Notes:**\n",
    "- Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized, which might lead to the model not performing optimally until it is trained further.\n",
    "\n",
    "### **CONCLUSIONS**\n",
    "\n",
    "- **Precision (P):** 0.9085, meaning that 90.85% of the words in the generated summaries are also present in the reference summaries. This indicates that the generated summaries are highly precise, with a minimal amount of irrelevant information.\n",
    "\n",
    "- **Recall (R):** 0.9244, meaning that 92.44% of the words in the reference summaries are also found in the generated summaries. This suggests that the model is capturing the majority of the essential information from the reference summaries.\n",
    "\n",
    "- **F1 score:** 0.9164, which is the harmonic mean of Precision and Recall. This score balances both Precision and Recall, indicating that the model performs well in generating summaries that are both accurate (Precision) and comprehensive (Recall).\n",
    "\n",
    "### **SUMMARY** \n",
    "- The high **F1 score** shows that the model is effectively summarizing the content by maintaining a balance between including relevant information and avoiding unnecessary or irrelevant details.\n",
    "\n",
    "- The **Precision** and **Recall** values suggest that the model is capturing a substantial portion of the content while keeping the information concise and relevant. The performance is impressive, even considering that some model weights were randomly initialized during the evaluation.\n",
    "\n",
    "- While the model performs well, further fine-tuning, especially for the pooler layer, might lead to slight improvements in semantic understanding and overall summary quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluating the model using METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from itertools import chain\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_summary(entry):\n",
    "    try:\n",
    "        post_summary_json = entry.get('post_summary', '{}')\n",
    "        post_summary_dict = json.loads(post_summary_json)\n",
    "        title = post_summary_dict.get('title', '')\n",
    "        selftext = post_summary_dict.get('selftext', '')\n",
    "        text = f\"{title} {selftext}\"\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        return tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing entry: {e}\")\n",
    "        print(f\"Entry content: {str(entry)[:200]}...\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/summary.json', 'r') as file:\n",
    "    generated_summaries = json.load(file)   \n",
    "\n",
    "with open('../data/reference.json', 'r') as file:\n",
    "    reference_summaries = json.load(file)\n",
    "    \n",
    "with open('../data/reference2.json', 'r') as file:\n",
    "    reference_summaries2 = json.load(file)\n",
    "\n",
    "with open('../data/reference3.json', 'r') as file:\n",
    "    reference_summaries3 = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_tokens = tokenize_summary(generated_summaries)\n",
    "reference_tokens = tokenize_summary(reference_summaries)\n",
    "reference_tokens2 = tokenize_summary(reference_summaries2)\n",
    "reference_tokens3 = tokenize_summary(reference_summaries3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_tokens_all = reference_tokens + reference_tokens2 + reference_tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_summaries)\n",
    "print(reference_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened reference tokens: ['Best', 'Ġbudget', '-', 'friendly', 'ĠEr', 'g', 'onomic', 'ĠChair', 'Ġwith', 'ĠAdjust', 'able', 'ĠHead', 'rest', ',', 'ĠArm', 'rest', 'Ġand', 'ĠL', 'umb', 'ar', 'ĠPill', 'ow', 'ĠThe', 'Ġuser', 'Ġis', 'Ġlooking', 'Ġfor', 'Ġa', 'Ġnew', 'Ġerg', 'onomic', 'Ġchair', 'Ġaround', 'ĠP', '3', ',', '000', '-', 'P', '4', ',', '000', 'Ġwith', 'Ġan', 'Ġadjustable', 'Ġhead', 'rest', ',', 'Ġarm', 'rest', 'Ġand', 'Ġl', 'umb', 'ar', 'Ġpillow', 'Ġmade', 'Ġwith', 'Ġmesh', 'Ġmaterial', '.', 'Best', 'Ġbudget', '-', 'friendly', 'ĠEr', 'g', 'onomic', 'ĠChair', 'Ġwith', 'ĠAdjust', 'able', 'ĠHead', 'rest', ',', 'ĠArm', 'rest', 'Ġand', 'ĠL', 'umb', 'ar', 'ĠPill', 'ow', 'Ġfor', 'ĠW', 'FH', 'ĠThe', 'Ġuser', 'Ġis', 'Ġlooking', 'Ġfor', 'Ġa', 'Ġnew', 'Ġerg', 'onomic', 'Ġchair', 'Ġaround', 'ĠP', '3', ',', '000', '-', 'P', '4', ',', '000', 'Ġwith', 'Ġan', 'Ġadjustable', 'Ġhead', 'rest', ',', 'Ġarm', 'rest', 'Ġand', 'Ġl', 'umb', 'ar', 'Ġpillow', 'Ġmade', 'Ġwith', 'Ġmesh', 'Ġmaterial', 'Ġfor', 'ĠW', 'FH', '.', 'Best', 'Ġbudget', '-', 'friendly', 'ĠMesh', 'ĠEr', 'g', 'onomic', 'ĠCh', 'airs', 'Ġwith', 'ĠAdjust', 'able', 'ĠHead', 'rest', ',', 'ĠArm', 'rest', 'Ġand', 'ĠL', 'umb', 'ar', 'ĠPill', 'ow', 'Ġfor', 'ĠW', 'FH', 'ĠThe', 'Ġuser', 'Ġis', 'Ġasking', 'Ġfor', 'Ġsuggestions', 'Ġfor', 'Ġa', 'Ġnew', 'Ġmesh', 'Ġerg', 'onomic', 'Ġchair', 'Ġaround', 'ĠP', '3', ',', '000', '-', 'P', '4', ',', '000', 'Ġwith', 'Ġan', 'Ġadjustable', 'Ġhead', 'rest', ',', 'Ġarm', 'rest', 'Ġand', 'Ġl', 'umb', 'ar', 'Ġpillow', 'Ġfor', 'ĠW', 'FH', '.']\n"
     ]
    }
   ],
   "source": [
    "flattened_reference_tokens = list(chain.from_iterable(reference_tokens))\n",
    "print(\"Flattened reference tokens:\", flattened_reference_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR score: 0.24830702899410625\n"
     ]
    }
   ],
   "source": [
    "meteor_score_value = meteor_score([reference_tokens_all], generated_tokens)\n",
    "\n",
    "print(f\"METEOR score: {meteor_score_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
