{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK IS FOR PREPARING, CHECKING AND CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset by scraping the top 100 reddit posts in a subreddit (in this case r/PHbuildapc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = os.getenv('CLIENT_ID'),\n",
    "    client_secret = os.getenv('CLIENT_SECRET'),\n",
    "    user_agent = \"retldr\"\n",
    ")\n",
    "\n",
    "data = []\n",
    "subreddit = reddit.subreddit(\"PHbuildapc\") # subreddit of choice \n",
    "\n",
    "for submission in subreddit.top(limit=100): # only fetch the top 100 posts\n",
    "    post = {\n",
    "        \"title\": submission.title,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"comments\": [\n",
    "            comment.body for comment in submission.comments\n",
    "            if isinstance(comment, praw.models.Comment)\n",
    "        ]\n",
    "    }\n",
    "    data.append(post)\n",
    "\n",
    "# save dataset to json\n",
    "with open(\"reddit_data.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset (json) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_data.json', 'r+') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many summaries have been added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 posts have summaries added to it\n",
      "posts with index [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30] have summaries added to it\n"
     ]
    }
   ],
   "source": [
    "summary_counter = 0\n",
    "post_w_summary = []\n",
    "for i in data:\n",
    "    if \"summary\" in i:\n",
    "        summary_counter+=1\n",
    "        post_w_summary.append(i[\"index\"])\n",
    "\n",
    "print(f\"{summary_counter} posts have summaries added to it\")\n",
    "print(f\"posts with index {post_w_summary} have summaries added to it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add index numbers to the dataset ( run only once )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index numbers added and saved to json file.\n"
     ]
    }
   ],
   "source": [
    "index_num = 0\n",
    "for i in data:\n",
    "    i['index'] = index_num\n",
    "    index_num += 1\n",
    "\n",
    "with open('reddit_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"index numbers added and saved to json file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to automate adding summaries to each post entry in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 40\n",
    "summary = {\n",
    "  \"title\": \"We are flooded with USED GPUs right now, please be careful ✌️\",\n",
    "  \"selftext\": \"The user discusses the increasing availability of used GPUs, especially those released by miners in the Philippines. They highlight the current prices of used GPUs, such as the 2060 costing 9k PHP and the 2060 Super costing 12k PHP. The user advises potential buyers to thoroughly test used GPUs, checking both their physical appearance and performance under load using stress testing apps like Furmark and Unigine Heaven. They mention that this may be the best time to buy a used GPU if you're willing to take the risk.\",\n",
    "  \"links_to_deals\": [\n",
    "    {\n",
    "      \"product\": \"Used 2060\",\n",
    "      \"price\": \"9k PHP\",\n",
    "      \"link\": \"https://imgur.com/Bj0jwjt\"\n",
    "    },\n",
    "    {\n",
    "      \"product\": \"Used 2060 Super\",\n",
    "      \"price\": \"12k PHP\",\n",
    "      \"link\": \"https://www.lazada.com.ph/products/used-colorful-geforce-rtx-2060-super-8g-graphics-card-sp-2176-8gb-gddr6-256bit-1470mhz-1650mhz-dphddvi-video-card-i3342718056-s17004144960.html?clickTrackInfo=query%253A2060super%253Bnid%253A3342718056%253Bsrc%253ALazadaMainSrp%253Brn%253A42ac091acc1b5be0eb1f7fa0438df410%253Bregion%253Aph%253Bsku%253A3342718056_PH%253Bprice%253A12799.00%253Bclient%253Adesktop%253Bsupplier_id%253A100067020%253Basc_category_id%253A5157%253Bitem_id%253A3342718056%253Bsku_id%253A17004144960%253Bshop_id%253A86237&freeshipping=1&fs_ab=2&fuse_fs=1&search=1&spm=a2o4l.searchlist.list.i68.48b27a08Cb5SW7\"\n",
    "    }\n",
    "  ],\n",
    "  \"comments\": [\n",
    "    {\n",
    "      \"topic\": \"Wear and tear of mining GPUs\",\n",
    "      \"comment\": \"Users express concerns about the potential wear and tear of GPUs used for mining. They mention that mining GPUs often experience more stress due to prolonged 24/7 use, which could lead to issues like broken capacitors and voltage regulators.\"\n",
    "    },\n",
    "    {\n",
    "      \"topic\": \"Risk vs reward of buying mining GPUs\",\n",
    "      \"comment\": \"Some users mention that while buying mining GPUs can be risky, there are instances where the cards may still function well, especially if they are priced significantly lower than new ones.\"\n",
    "    },\n",
    "    {\n",
    "      \"topic\": \"Alternative stress-testing apps\",\n",
    "      \"comment\": \"Others suggest using different stress-testing apps like Unigine Valley and 3DMark Firestrike/Timespy to check GPU performance.\"\n",
    "    },\n",
    "    {\n",
    "      \"topic\": \"Used 6700XT deal\",\n",
    "      \"comment\": \"One user considers buying a used 6700XT with more than two years of warranty left for only $180 as an appealing option.\"\n",
    "    }\n",
    "  ],\n",
    "  \"sentiment\": \"The post encourages cautiousness when buying used GPUs, particularly those used for mining. The tone is informative and emphasizes the importance of testing and evaluating the product thoroughly before making a purchase. There is a sense of risk associated with buying used GPUs, but some users also share good deals they have found.\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in data:\n",
    "    if i[\"index\"] == index:\n",
    "        i[\"summary\"] = summary\n",
    "        \n",
    "with open('reddit_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
