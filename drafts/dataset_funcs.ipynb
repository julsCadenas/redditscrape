{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS NOTEBOOK IS FOR PREPARING, CHECKING AND CLEANING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import praw\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dataset by scraping the top 100 reddit posts in a subreddit (in this case r/PHbuildapc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id = os.getenv('CLIENT_ID'),\n",
    "    client_secret = os.getenv('CLIENT_SECRET'),\n",
    "    user_agent = \"retldr\"\n",
    ")\n",
    "\n",
    "data = []\n",
    "subreddit = reddit.subreddit(\"PHbuildapc\") # subreddit of choice \n",
    "\n",
    "for submission in subreddit.top(limit=100): # only fetch the top 100 posts\n",
    "    post = {\n",
    "        \"title\": submission.title,\n",
    "        \"selftext\": submission.selftext,\n",
    "        \"comments\": [\n",
    "            comment.body for comment in submission.comments\n",
    "            if isinstance(comment, praw.models.Comment)\n",
    "        ]\n",
    "    }\n",
    "    data.append(post)\n",
    "\n",
    "# save dataset to json\n",
    "with open(\"reddit_data.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the dataset (json) file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_data.json', 'r+') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many summaries have been added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 posts have summaries added to it\n",
      "posts with index [0, 1, 2, 3, 4, 5] have summaries added to it\n"
     ]
    }
   ],
   "source": [
    "summary_counter = 0\n",
    "post_w_summary = []\n",
    "for i in data:\n",
    "    if \"summary\" in i:\n",
    "        summary_counter+=1\n",
    "        post_w_summary.append(i[\"index\"])\n",
    "\n",
    "print(f\"{summary_counter} posts have summaries added to it\")\n",
    "print(f\"posts with index {post_w_summary} have summaries added to it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add index numbers to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index numbers added and saved to json file.\n"
     ]
    }
   ],
   "source": [
    "index_num = 0\n",
    "for i in data:\n",
    "    i['index'] = index_num\n",
    "    index_num += 1\n",
    "\n",
    "with open('reddit_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n",
    "\n",
    "print(\"index numbers added and saved to json file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to automate adding summaries to each post entry in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6\n",
    "summary = \"A comprehensive Reddit post outlines where to buy PC parts in the Philippines, comparing physical and online stores. For physical stores, key recommendations include EasyPC, PCExpress, PCHub, and DynaQuestPC, highlighting benefits like testing products before purchase and immediate customer service. For online shopping, the post recommends trusted sellers like tech2027, Digiserve, and Zenith Avenue Era on platforms like Lazada. The community's comments validate many recommendations while adding suggestions like PC Worth, IT World, and regional stores. Users share personal experiences, with most emphasizing the importance of warranty service, customer support, and price comparisons between physical and online retailers.\"\n",
    "\n",
    "for i in data:\n",
    "    if i[\"index\"] == index:\n",
    "        i[\"summary\"] = summary\n",
    "\n",
    "with open('reddit_data.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
